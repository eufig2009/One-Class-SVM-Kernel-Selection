import numpy as np
import pylab as pl
import pandas as pd

from sklearn.metrics import pairwise_distances
from scipy.linalg import cholesky
from sklearn.svm import SVDD
from sklearn.utils import check_random_state
from scipy.spatial import cKDTree
from sklearn.utils import array2d
nu = 0.1


def generate_synthetic_data(size, dim=2, centroid_count=2, centroid_dispersion=1):
    size = int(size)
    centroids = np.random.rand(centroid_count, dim) * 10 - 5
    all_data_part = []
    for point in centroids:
        covariation_matrix = np.random.randn(dim, dim)
        covariation_matrix = np.dot(covariation_matrix, covariation_matrix.T)
        covariation_matrix = cholesky(covariation_matrix)
        data_part = np.random.randn(int(size / centroid_count), dim)
        data_part = np.dot(data_part, covariation_matrix) + point
        all_data_part.append(data_part)
    data = np.concatenate(all_data_part, axis=0)
    return data


def generate_outlier(size, dim=2, space=1000):
    size = int(size)
    outliers = (np.random.rand(size, dim) - 0.5) * space * 2
    return outliers

def generate_dataset(size, outliers_part=0.12, dim=2, centroid_count=2, centroid_dispersion=1):
    inliers = generate_synthetic_data(size * (1 - outliers_part), 
                                               dim, centroid_count, 
                                               centroid_dispersion)
    outliers = generate_outlier(size*outliers_part, dim, space=centroid_dispersion*50)
    data = np.concatenate([inliers, outliers], axis=0)
    data = pd.DataFrame(data)
    data['label'] = ['target'] * len(inliers) + ['outlier'] * len(outliers)
    return data

def kernel_metric(clf, train_x):
    gamma = clf.gamma
    distance = pairwise_distances(train_x)
    distance = distance * distance
    kernel_matrix = np.exp(-gamma * distance)
    kernel_matrix -= np.eye(kernel_matrix.shape[0])
    reguarization = 0.001
    return -np.var(kernel_matrix) / (np.mean(kernel_matrix) + reguarization)


def support_vectors_metric(clf, train_x):
    clf.fit(train_x)
    prediction = clf.predict(train_x)
    out_of_class_fraction = np.mean(prediction == -1)
    support_vectors_fraction = float(len(clf.support_vectors_)) / len(train_x)
    metric = (out_of_class_fraction - nu) ** 2
    metric += (support_vectors_fraction - nu) ** 2
    return metric

def model_selection(data, all_gammas, selection_technic):
    metrics = np.zeros(all_gammas.shape[0])
    for index, gamma in enumerate(all_gammas):
        clf = SVDD(C = 1.0 / 100, gamma=gamma, kernel='rbf')
        metrics[index] = selection_technic(clf, data)
    return metrics

def SMOTE(X, n_samples, k=5, dist_power=2, multiple=False, sample_weight=None,
          random_state=None):
    """
    Returns n_samples of synthetic samples from X generated by SMOTE.

    Parameters
    ----------
    X : array-like, shape = [n_minority_samples, n_features]
        Holds the minority samples
    n_samples : int
        Number of new synthetic samples.
    k : int
        Number of nearest neighbours.
    dist_power : float, int
        Positive power in ditance metrics.
    random_state : None or int
        Seed for random generator.


    Returns
    -------
    smoted_X : array, shape = [n_samples, n_features]
        Synthetic samples
    """
    
    if type(X) is pd.core.frame.DataFrame:
        X = X.values

    rng = check_random_state(random_state)
    n_minor, n_features = X.shape
    k = min([n_minor - 1, k])

    # Learn nearest neighbours
    nn_tree = cKDTree(X)

    if multiple:
        smoted_X = X.copy()
        if sample_weight is not None:
            weight_smoted = sample_weight.copy()
        nn_dist, nn_idx = nn_tree.query(smoted_X, k=k + 1, p=dist_power)
        nn_idx = nn_idx[:, 1:]
        for i in xrange(n_samples):
            start_idx = rng.choice(len(smoted_X))
            start = smoted_X[start_idx, :]
            end_idx = nn_idx[start_idx, rng.choice(k)]
            end = smoted_X[end_idx, :]
            shift = rng.rand()
            new_point = [start * shift + end * (1. - shift)]
            new_nn_idx = np.argsort(
                distance_matrix(smoted_X, new_point,
                                p=dist_power).T)[::-1][0][:k]
            smoted_X = np.vstack((smoted_X, new_point))
            nn_idx = np.vstack((nn_idx, new_nn_idx))
            if sample_weight is not None:
                weight_smoted = \
                    np.concatenate((weight_smoted,
                                    weight_smoted[start_idx] * shift +
                                    (1. - shift) * weight_smoted[end_idx]))

    else:
        start_indices = rng.choice(len(X), size=(n_samples,))
        starts = X[start_indices, :]
        nn_dists, nn_idx = nn_tree.query(starts, k=k + 1, p=dist_power)
        end_indices = nn_idx[np.arange(n_samples),
                             rng.choice(np.arange(1, k + 1), n_samples)]
        ends = X[end_indices, :]
        shifts = rng.rand(n_samples)
        smoted_X = starts * np.repeat(array2d(shifts).T, n_features, axis=1) \
            + ends * np.repeat(array2d(1. - shifts).T, n_features, axis=1)
        smoted_X = np.vstack((X, smoted_X))
        if sample_weight is not None:
            weight_smoted = sample_weight[start_indices] * shifts\
                + (1. - shifts) * sample_weight[end_indices]
    if sample_weight is None:
        return smoted_X
    else:
        return smoted_X, np.concatenate((sample_weight, weight_smoted))


def validate_classifier_by_random_points(clf, train_x, size=10000):
    clf.fit(train_x)
    positive_points = SMOTE(train_x, k=5,n_samples=size)
    negative_points = np.random.rand(size, train_x.shape[1])
    negative_points *= np.std(train_x)
    error = np.mean(clf.predict(positive_points) == -1)
    error += np.mean(clf.predict(negative_points) == 1)
    return error

def combinatorial_dimension_metric(clf, train_x):
    clf.fit(train_x)
    prediction = clf.decision_function(train_x)
    negative_marks = prediction < 0
    prediction = prediction[negative_marks]
    distance = prediction.min()
    radius = calculate_radius(clf)
    radius += abs(distance)
    return abs(distance) / radius

def rbf_kernel(X, Y, gamma):
    distance_matrix = pairwise_distances(X, Y) ** 2
    #print "Gamma = ", gamma
    distance_matrix = np.exp(-1 * gamma * distance_matrix)
    return distance_matrix

def calculate_radius(clf):
    all_support_vectors = clf.support_vectors_
    first_support_vector = clf.support_vectors_[0, :]
    dual_coef = clf.dual_coef_
    gamma = clf.gamma
    test_vector_norm = 1
    second_part = rbf_kernel(all_support_vectors, first_support_vector, gamma)
    #print "second_vector", second_part
    second_part = np.dot(dual_coef, second_part)
    #print second_part
    third_part = np.dot(dual_coef, np.dot(rbf_kernel(all_support_vectors, all_support_vectors, gamma), dual_coef.T))[0, 0]
    radius = (test_vector_norm - 2 * second_part + third_part)
    return radius[0, 0]

all_gammas = np.logspace(-10, 10, 50)
data = generate_dataset(1000).iloc[:, :-1]
values = model_selection(data, all_gammas, combinatorial_dimension_metric)
pl.plot(all_gammas, values)
pl.xscale('log')
pl.yscale('log')
pl.xlabel('Gammas')
pl.ylabel('Score')
pl.show()