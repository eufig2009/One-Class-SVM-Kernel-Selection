from sklearn.svm import SVDD
from sklearn.metrics import pairwise_distances
import pandas as pd
from random import sample
import sys
import os
from sklearn.cross_validation import KFold
from scipy.linalg import cholesky
from sklearn.cross_validation import train_test_split 
from sklearn.metrics import roc_auc_score
from sklearn.utils import check_random_state
from scipy.spatial import cKDTree
from sklearn.utils import array2d
import time
import numpy as np
import pylab as pl


def SMOTE(X, n_samples, k=5, dist_power=2, multiple=False, sample_weight=None,
          random_state=None):
    """
    Returns n_samples of synthetic samples from X generated by SMOTE.

    Parameters
    ----------
    X : array-like, shape = [n_minority_samples, n_features]
        Holds the minority samples
    n_samples : int
        Number of new synthetic samples.
    k : int
        Number of nearest neighbours.
    dist_power : float, int
        Positive power in ditance metrics.
    random_state : None or int
        Seed for random generator.


    Returns
    -------
    smoted_X : array, shape = [n_samples, n_features]
        Synthetic samples
    """
    
    if type(X) is pd.core.frame.DataFrame:
        X = X.values

    rng = check_random_state(random_state)
    n_minor, n_features = X.shape
    k = min([n_minor - 1, k])

    # Learn nearest neighbours
    nn_tree = cKDTree(X)

    if multiple:
        smoted_X = X.copy()
        if sample_weight is not None:
            weight_smoted = sample_weight.copy()
        nn_dist, nn_idx = nn_tree.query(smoted_X, k=k + 1, p=dist_power)
        nn_idx = nn_idx[:, 1:]
        for i in xrange(n_samples):
            start_idx = rng.choice(len(smoted_X))
            start = smoted_X[start_idx, :]
            end_idx = nn_idx[start_idx, rng.choice(k)]
            end = smoted_X[end_idx, :]
            shift = rng.rand()
            new_point = [start * shift + end * (1. - shift)]
            new_nn_idx = np.argsort(
                distance_matrix(smoted_X, new_point,
                                p=dist_power).T)[::-1][0][:k]
            smoted_X = np.vstack((smoted_X, new_point))
            nn_idx = np.vstack((nn_idx, new_nn_idx))
            if sample_weight is not None:
                weight_smoted = \
                    np.concatenate((weight_smoted,
                                    weight_smoted[start_idx] * shift +
                                    (1. - shift) * weight_smoted[end_idx]))

    else:
        start_indices = rng.choice(len(X), size=(n_samples,))
        starts = X[start_indices, :]
        nn_dists, nn_idx = nn_tree.query(starts, k=k + 1, p=dist_power)
        end_indices = nn_idx[np.arange(n_samples),
                             rng.choice(np.arange(1, k + 1), n_samples)]
        ends = X[end_indices, :]
        shifts = rng.rand(n_samples)
        smoted_X = starts * np.repeat(array2d(shifts).T, n_features, axis=1) \
            + ends * np.repeat(array2d(1. - shifts).T, n_features, axis=1)
        smoted_X = np.vstack((X, smoted_X))
        if sample_weight is not None:
            weight_smoted = sample_weight[start_indices] * shifts\
                + (1. - shifts) * sample_weight[end_indices]
    if sample_weight is None:
        return smoted_X
    else:
        return smoted_X, np.concatenate((sample_weight, weight_smoted))


def generate_synthetic_data(size, dim=2, centroid_count=2, centroid_dispersion=1):
    size = int(size)
    centroids = np.random.rand(centroid_count, dim) * 10 - 5
    all_data_part = []
    for point in centroids:
        covariation_matrix = np.random.randn(dim, dim)
        covariation_matrix = np.dot(covariation_matrix, covariation_matrix.T)
        covariation_matrix = cholesky(covariation_matrix)
        data_part = np.random.randn(int(size / centroid_count), dim)
        data_part = np.dot(data_part, covariation_matrix) + point
        all_data_part.append(data_part)
    data = np.concatenate(all_data_part, axis=0)
    return data


def generate_outlier(size, dim=2, space=2):
    size = int(size)
    outliers = (np.random.rand(size, dim) - 0.5) * space * 2
    return outliers


def generate_dataset(size, outliers_part=0.12, dim=2, centroid_count=2, centroid_dispersion=1):
    inliers = generate_synthetic_data(size * (1 - outliers_part), 
                                               dim, centroid_count, 
                                               centroid_dispersion)
    space = inliers.max() - inliers.min()
    outliers = generate_outlier(size*outliers_part, dim, space=space)
    data = np.concatenate([inliers, outliers], axis=0)
    data = pd.DataFrame(data)
    data['label'] = ['target'] * len(inliers) + ['outlier'] * len(outliers)
    return data


def validate_classifier_by_random_points(clf, train_x, size=10000):
    clf.fit(train_x)
    positive_points = SMOTE(train_x, k=5,n_samples=size)
    negative_points = np.random.rand(size, train_x.shape[1])
    negative_points *= np.std(train_x)
    error = np.mean(clf.predict(positive_points) == -1)
    error += np.mean(clf.predict(negative_points) == 1)
    return error


def rbf_kernel(X, Y, gamma):
    distance_matrix = pairwise_distances(X, Y) ** 2
    #print "Gamma = ", gamma
    distance_matrix = exp(-1 * gamma * distance_matrix)
    return distance_matrix


def calculate_radius(clf):
    all_support_vectors = clf.support_vectors_
    first_support_vector = clf.support_vectors_[0, :]
    dual_coef = clf.dual_coef_
    gamma = clf.gamma
    test_vector_norm = 1
    second_part = rbf_kernel(all_support_vectors, first_support_vector, gamma)
    #print "second_vector", second_part
    second_part = dot(dual_coef, second_part)
    #print second_part
    third_part = dot(dual_coef, dot(rbf_kernel(all_support_vectors, all_support_vectors, gamma), dual_coef.T))[0, 0]
    radius = (test_vector_norm - 2 * second_part + third_part)
    return radius[0, 0]


def combinatorial_dimension_metric(clf, train_x):
    clf.fit(train_x)
    prediction = clf.decision_function(train_x)
    negative_marks = prediction < 0
    prediction = prediction[negative_marks]
    distance = prediction.min()
    radius = calculate_radius(clf)
    radius += abs(distance)
    return abs(distance) / radius


def kernel_metric(clf, train_x):
    gamma = clf.gamma
    distance = pairwise_distances(train_x)
    distance = distance * distance
    kernel_matrix = exp(-gamma * distance)
    kernel_matrix -= eye(kernel_matrix.shape[0])
    reguarization = 0.001
    return -var(kernel_matrix) / (mean(kernel_matrix) + reguarization)


def support_vectors_metric(clf, train_x):
    clf.fit(train_x)
    prediction = clf.predict(train_x)
    out_of_class_fraction = mean(prediction == -1)
    support_vectors_fraction = float(len(clf.support_vectors_)) / len(train_x)
    metric = (out_of_class_fraction - nu) ** 2
    metric += (support_vectors_fraction - nu) ** 2
    return metric


def split_anomaly_normal_data(data, outliers_fraction=0.1):
    outliers = data.query("label == 'outlier'")
    inliers = data.query("label == 'target'")
    outliers_count = int(len(inliers) * outliers_fraction / (1 - outliers_fraction))
    if outliers_count > len(outliers):
        raise ValueError("There are no so many outliers")
    outliers_rows = sample(outliers.index, outliers_count)
    selected_outliers = outliers.ix[outliers_rows]
    return inliers.iloc[:, :-1], selected_outliers.iloc[:, :-1]


def split_data_set(data, parts_count=3):
    all_parts = []
    kfold = KFold(len(data), n_folds=parts_count, shuffle=True)
    for train, test in kfold:
        all_parts.append(data.iloc[test, :])
    return all_parts


def validate_gamma(train, test_normal, test_anomaly, gamma, nu=0.1):
    C = 1.0 / (nu * (len(train)))
    clf = SVDD(kernel='rbf', gamma=gamma, C=C)
    #clf.fit(np.random.randn(10000, 4))
    clf.fit(train)
    normal_data_prediction = clf.decision_function(test_normal)
    anomaly_data_prediction = clf.decision_function(test_anomaly)
    normal_data_error = np.mean(normal_data_prediction < 0)
    anomaly_data_error = np.mean(anomaly_data_prediction > 0)
    true_labels = [1] * len(test_normal) + [-1] * len(test_anomaly)
    decision_values = np.concatenate([normal_data_prediction, anomaly_data_prediction], axis=0)
    auc_score = roc_auc_score(true_labels, decision_values)
    return normal_data_error, anomaly_data_error, auc_score


def process_data_set(data):
    normal_data, anomaly_data = split_anomaly_normal_data(data)
    selection_technics = [validate_classifier_by_random_points, kernel_metric, support_vectors_metric, combinatorial_dimension_metric]
    normal_train, normal_validate, normal_test = split_data_set(normal_data, 3)
    anomaly_train, anomaly_validate, anomaly_test = split_data_set(anomaly_data, 3)
    results = {}
    all_gammas = np.logspace(-10, 10, 50)
    for technic in selection_technics:
        start = time.time()
        metrics = model_selection(concatenate([normal_validate, anomaly_validate]), all_gammas, technic)
        duration = time.time() - start
        best_index = np.argmin(metrics)
        best_score = metrics[best_index]
        all_best_gamma = metrics == best_score
        possible_results = all_gammas[all_best_gamma]
        best_gamma = np.max(possible_results)
        fn, fp, auc = validate_gamma(concatenate([normal_train, anomaly_train]), normal_test, anomaly_test, best_gamma)
        results[str(technic).split()[1]] = {'false negative': fn, 'false positive': fp, 'auc': auc, 'time':duration}
    return results


if __name__ == '__main__':
    data = generate_dataset(size=1000)
    pl.scatter(data.iloc[:, 0], data.iloc[:, 1])
    pl.show()